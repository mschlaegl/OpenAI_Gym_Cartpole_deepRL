{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5f0392",
   "metadata": {},
   "source": [
    "# OpenAI Gym -- Solved Cartpole using DeepRL\n",
    "\n",
    "Solution for OpenAI Gym's Cartpole environment using deep reeinforcement learning.\n",
    "(C) 2022 Manfred SCHLAEGL <manfred.schlaegl@gmx.at>\n",
    "\n",
    "Code inspired by [(527) Deep Q-Network Training Code - Reinforcement Learning Code Project - YouTube](https://www.youtube.com/watch?v=ewRw996uevM&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=18)\n",
    "but strongly reworked (e.g. Cartpole observations instead of Images; Implemented load, save, play, ...)\n",
    "\n",
    "OpenAI Gym Cartpole Environment: https://www.gymlibrary.ml/environments/classic_control/cart_pole/\n",
    "\n",
    "## Requirements\n",
    " * Standard python packages (numpy, ...)\n",
    " * Matplolib: pip install Matplotlib\n",
    " * OpenAI Gym: pip install gym[all]\n",
    " * Pygame: pip install pygame\n",
    " * PyTorch: pip install torch\n",
    " * PyTorch Torchvision: pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576642df",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdd77a",
   "metadata": {},
   "source": [
    "### Imports and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1637e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyternotebook/virtualenv/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py:228: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  interpolation: int = Image.BILINEAR,\n",
      "/opt/jupyternotebook/virtualenv/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py:295: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  interpolation: int = Image.NEAREST,\n",
      "/opt/jupyternotebook/virtualenv/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py:328: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  interpolation: int = Image.BICUBIC,\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#from gym import wrappers\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f661a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "torchdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef2466",
   "metadata": {},
   "source": [
    "### Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac904a0e",
   "metadata": {},
   "source": [
    "#### DQN: Model for policy (and target) network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a29f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN extends nn.Module\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \"\"\"\n",
    "        model constructor\n",
    "\n",
    "        :param n_inputs: number of inputs\n",
    "        :param n_outputs: number of outputs\n",
    "        \"\"\" \n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=n_inputs, out_features=24)\n",
    "        self.out = nn.Linear(in_features=24, out_features=n_outputs)\n",
    "\n",
    "    \n",
    "    # overloaded\n",
    "    def forward(self, inputs):\n",
    "        t = inputs.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "\n",
    "    \n",
    "    def clone(self, torchdevice):\n",
    "        \"\"\"\n",
    "        clone this model\n",
    "\n",
    "        :torchdevice:\n",
    "        :return: clone of current model\n",
    "        \"\"\"\n",
    "        clone = DQN(self.n_inputs, self.n_outputs).to(torchdevice)\n",
    "        clone.load_state_dict(self.state_dict())\n",
    "        return clone\n",
    "    \n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        save the model to given file\n",
    "\n",
    "        :filename: file to save the model to\n",
    "        \"\"\" \n",
    "        torch.save(self.state_dict(), filename)\n",
    "        \n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        load the model from given file\n",
    "\n",
    "        :filename: file to save the model to\n",
    "        :return: False if file does not exist\n",
    "        \"\"\"\n",
    "        if not exists(filename):\n",
    "            return False\n",
    "        \n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336375b5",
   "metadata": {},
   "source": [
    "#### Replay Memory for Experience Replay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20391ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python tuple with named fields\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        initialize with given capacity\n",
    "\n",
    "        :capacity: capacity of the ReplayMemory\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        self.push_idx = 0\n",
    "       \n",
    "    \n",
    "    def push(self, experience):\n",
    "        \"\"\"\n",
    "        push a new experience\n",
    "        if the memory is full we push in round robin fashion\n",
    "\n",
    "        :experience: Experience to push\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_idx] = experience\n",
    "        self.push_idx += 1\n",
    "        if self.push_idx >= self.capacity:\n",
    "            self.push_idx = 0\n",
    "        self.push_count += 1\n",
    "        \n",
    "        \n",
    "    def nElements(self):\n",
    "        \"\"\"\n",
    "        Number of experiences in ReplayMemory\n",
    "\n",
    "        :return: number of experiences in ReplayMemory\n",
    "        \"\"\"\n",
    "        return len(self.memory)\n",
    "        \n",
    "        \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Enough experiences in ReplayMemory to return batch_size\n",
    "\n",
    "        :batch_size: size of the batch\n",
    "        :return: True if number of experiences > batch_size\n",
    "        \"\"\"\n",
    "        return self.nElements() >= batch_size\n",
    "    \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        get a random batch of experiences\n",
    "\n",
    "        :batch_size: size of the batch\n",
    "        :return: random batch (None if not enough samples)\n",
    "        \"\"\"\n",
    "        if not self.can_provide_sample(batch_size):\n",
    "            return None\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3047cb",
   "metadata": {},
   "source": [
    "#### Agent (and Action Selection Strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec22ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon Greedy Strategy (for Training)\n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)\n",
    "\n",
    "# Exploitation Strategy (for Playing)\n",
    "class ExploitationStrategy():\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Agent (Action Selection)\n",
    "class Agent():\n",
    "    def __init__(self, torchdevice, strategy, n_actions):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.n_actions = n_actions\n",
    "        self.torchdevice = torchdevice\n",
    "        \n",
    "    def select_action(self, state, policy_net):\n",
    "        \"\"\"\n",
    "        select action w.r.t. given state and configured\n",
    "        strategy\n",
    "\n",
    "        :state: current state\n",
    "        :policy_net: the policy network\n",
    "        :return: action\n",
    "        \"\"\"\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        if rate > random.random():\n",
    "            #print('explore')\n",
    "            action = random.randrange(self.n_actions)\n",
    "            return torch.tensor([action]).to(self.torchdevice)\n",
    "        else:\n",
    "            #print('exploit')\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.torchdevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40e6cd",
   "metadata": {},
   "source": [
    "#### Cartpole Environment Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212485eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnvManager():\n",
    "    def __init__(self, torchdevice):\n",
    "        self.torchdevice = torchdevice\n",
    "        # unwrapped gives access to behind the scene elements of env\n",
    "        self.env = gym.make('CartPole-v1').unwrapped\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.state = None\n",
    "        self.done = False\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "    \n",
    "    def n_observations(self):\n",
    "        return self.env.observation_space.shape[0]\n",
    "    \n",
    "    def n_actions(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        \"\"\"\n",
    "        perform action and return reward\n",
    "\n",
    "        :action: action to take\n",
    "        :return: reward\n",
    "        \"\"\"\n",
    "        # action is tensor -> item delivers value of tensor as python number\n",
    "        self.state, reward, self.done, _ = self.env.step(action.item())\n",
    "        return torch.tensor([np.float32(reward)], device=self.torchdevice)\n",
    "\n",
    "    def get_state(self):\n",
    "        if self.state is None or self.done:\n",
    "            # no action was done before\n",
    "            ret=torch.zeros(self.n_observations())\n",
    "        else:\n",
    "            ret=torch.tensor(self.state, device=self.torchdevice)\n",
    "        return ret.unsqueeze(0).to(self.torchdevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4fad6",
   "metadata": {},
   "source": [
    "#### QValue calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ec8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        # handling of final states\n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "            .max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(torchdevice)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b3088",
   "metadata": {},
   "source": [
    "#### Misc Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d71ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training ...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(values)\n",
    "    \n",
    "    moving_avg = get_moving_average(values, moving_avg_period)\n",
    "    plt.plot(moving_avg)\n",
    "    plt.pause(1)\n",
    "    print(\"Episode\", len(values), \"\\n\", \\\n",
    "         moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "\n",
    "def plot_results(values, moving_avg_period):\n",
    "    episode_best = np.argmax(values)\n",
    "    moving_avg_best = np.max(values)\n",
    "    print(\"best episode: \" + str(episode_best) + \" with reward \" + str(moving_avg_best))\n",
    "    plot(values, moving_avg_period)\n",
    "        \n",
    "def get_moving_average(values, period):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) < period:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n",
    "    \n",
    "    # periode slice and mean\n",
    "    moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "        .mean(dim=1).flatten(start_dim=0)\n",
    "    moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "    return moving_avg.numpy()\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "    return (t1, t2, t3, t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5119c29",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4b5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters for Training\n",
    "class HyperParameters():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # size of replay memory for experience replay\n",
    "        self.memory_size = 100000\n",
    "        \n",
    "        # batch_size (taken from replay memory)\n",
    "        self.batch_size = 256\n",
    "        \n",
    "        # RL gamma\n",
    "        self.gamma = 0.999\n",
    "        \n",
    "        # exploration rate (start, end, decay)\n",
    "        self.eps_start = 1\n",
    "        self.eps_end = 0.01\n",
    "        self.eps_decay = 0.001\n",
    "        \n",
    "        # episodes after which target_net = policy_net\n",
    "        self.target_update = 10\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = 0.001\n",
    "        \n",
    "        # number of episodes\n",
    "        self.n_episodes = 2000\n",
    "        \n",
    "        # maximum number of timesteps to play\n",
    "        self.max_timestep = 1000 # 500 is success in CartPole-v1\n",
    "        \n",
    "        # number of episodes used to calculate moving reward average\n",
    "        self.n_episodes_ravg = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f3623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(torchdevice, filename):\n",
    "    \"\"\"\n",
    "    load a policy network from given file\n",
    "\n",
    "    :torchdevice: torch.device(...)\n",
    "    :filename: filename\n",
    "    :return: polcy network (None if file not found)\n",
    "    \"\"\"\n",
    "\n",
    "    em = CartPoleEnvManager(torchdevice)\n",
    "    policy_net = DQN(\n",
    "        em.n_observations(),\n",
    "        em.n_actions()).to(torchdevice)\n",
    "    if not policy_net.load(filename):\n",
    "        return None\n",
    "    return policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "924ed53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(torchdevice, hpara, stat_update = 100, policy_net = None):\n",
    "    \"\"\"\n",
    "    train a policy network with given parameters\n",
    "\n",
    "    :torchdevice: torch.device(...)\n",
    "    :filename: filename\n",
    "    :stat_update: plot statistics after every stat_update episodes (default 100)\n",
    "    :policy_net: optional existing model to train (default None)\n",
    "    :return: (best found policy network, episode rewards)\n",
    "    \"\"\"\n",
    "        \n",
    "    # setup env and agent\n",
    "    em = CartPoleEnvManager(torchdevice)\n",
    "    agent = Agent(\n",
    "        torchdevice,\n",
    "        EpsilonGreedyStrategy(\n",
    "            hpara.eps_start,\n",
    "            hpara.eps_end,\n",
    "            hpara.eps_decay),\n",
    "        em.n_actions())\n",
    "    \n",
    "    # setup replay memory (for experience replay)\n",
    "    memory = ReplayMemory(hpara.memory_size)\n",
    "\n",
    "    # create new policy_net (if not given)\n",
    "    if policy_net == None:\n",
    "        policy_net = DQN(\n",
    "            em.n_observations(),\n",
    "            em.n_actions()).to(torchdevice)\n",
    "    policy_net.train() # switch to training mode\n",
    "    \n",
    "    # create target net\n",
    "    target_net = policy_net.clone(torchdevice)\n",
    "    target_net.eval() # only inference\n",
    "    \n",
    "    # setup optimizer\n",
    "    optimizer = optim.Adam(params=policy_net.parameters(), lr=hpara.lr)\n",
    "    \n",
    "    # track best episode\n",
    "    moving_avg_best = 0\n",
    "    episode_best = 0\n",
    "    policy_net_best = policy_net.clone(torchdevice)\n",
    "    policy_net_best.eval() # only inference    \n",
    "    \n",
    "    # track episode rewards\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # start training\n",
    "    print(\"0: \", end='')\n",
    "    for episode in range(hpara.n_episodes):\n",
    "\n",
    "        # start new episode\n",
    "        print(\".\", end='')\n",
    "        em.reset()\n",
    "        \n",
    "        state = em.get_state()\n",
    "        reward_sum = 0\n",
    "        for timestep in count():\n",
    "            \n",
    "            # make a move\n",
    "            action = agent.select_action(state, policy_net)\n",
    "            reward = em.take_action(action)\n",
    "            reward_sum += reward\n",
    "            next_state = em.get_state()\n",
    "            memory.push(Experience(state, action, next_state, reward))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # start training, after we have enough samples in replay memory\n",
    "            if memory.can_provide_sample(hpara.batch_size):\n",
    "                \n",
    "                # get batch of experiences\n",
    "                experiences = memory.sample(hpara.batch_size)\n",
    "                states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "                # calculate qvalues and loss\n",
    "                current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "                next_q_values = QValues.get_next(target_net, next_states)\n",
    "                target_q_values = (next_q_values * hpara.gamma) + rewards\n",
    "                loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "                \n",
    "                # pytorch saves grad of w and bias\n",
    "                optimizer.zero_grad()\n",
    "                # computes gradient w.r.t to bias and weights\n",
    "                loss.backward()\n",
    "                # update weight and bias from backward (connection to loss?)\n",
    "                optimizer.step()\n",
    "\n",
    "            \n",
    "            # end of episode?\n",
    "            if em.done or timestep >= hpara.max_timestep:\n",
    "                # statistics\n",
    "                episode_rewards.append(reward_sum)\n",
    "\n",
    "                # save best model found so far\n",
    "                moving_avg = get_moving_average(\n",
    "                    episode_rewards,\n",
    "                    para.n_episodes_ravg)[-1]\n",
    "                if moving_avg > moving_avg_best:\n",
    "                    print(\"\\nsave best episode: \" + str(episode_best) +\n",
    "                          \" with reward \" + str(moving_avg_best));\n",
    "                    if is_ipython: display.clear_output(wait=True)\n",
    "                    moving_avg_best = moving_avg\n",
    "                    episode_best = episode\n",
    "                    policy_net_best.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "                # update target_net\n",
    "                if episode % hpara.target_update == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "                # output statistics\n",
    "                if episode % stat_update == 0:\n",
    "                    print('\\nepisode: ' + str(episode))\n",
    "                    plot(episode_rewards, para.n_episodes_ravg)\n",
    "                    print(str(episode) + \": \", end='')\n",
    "\n",
    "                break\n",
    "\n",
    "    em.close()\n",
    "\n",
    "    # return best policy and statistics\n",
    "    policy_net_best.eval() # only inference    \n",
    "    return (policy_net_best, episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7dbc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(torchdevice, policy_net,\n",
    "         n_episodes_ravg = 100,\n",
    "         num_episodes = 1, \n",
    "         max_timestep = 1000):\n",
    "    \"\"\"\n",
    "    play the game using a given policy network\n",
    "\n",
    "    :torchdevice: torch.device(...)\n",
    "    :policy_net: policy network to use\n",
    "    :num_episodes: number of episodes to play (default 1)\n",
    "    :max_timestep: maximum timesteps to play (default 1000)\n",
    "    :n_episodes_ravg: number of episodes used to calculate moving reward average (for plot) (default 100)\n",
    "    :return: episode rewards\n",
    "    \"\"\"\n",
    "        \n",
    "    # setup env and agent\n",
    "    em = CartPoleEnvManager(torchdevice)\n",
    "    agent = Agent(torchdevice,\n",
    "                  ExploitationStrategy(),\n",
    "                  em.n_actions())\n",
    "\n",
    "    # track episode rewards\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        print('episode: ' + str(episode))\n",
    "\n",
    "        # start new episode\n",
    "        em.reset()\n",
    "\n",
    "        # play\n",
    "        state = em.get_state()\n",
    "        reward_sum = 0\n",
    "        for timestep in count():\n",
    "\n",
    "            # render output\n",
    "            em.render()\n",
    "\n",
    "            # make a move\n",
    "            action = agent.select_action(state, policy_net)\n",
    "            reward_sum += em.take_action(action)\n",
    "            state = em.get_state()\n",
    "\n",
    "            # end of episode?\n",
    "            if em.done or timestep >= max_timestep:\n",
    "                episode_rewards.append(reward_sum)\n",
    "                plot(episode_rewards, n_episodes_ravg)\n",
    "                break\n",
    "        \n",
    "    em.close()\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307656f",
   "metadata": {},
   "source": [
    "### Main Program\n",
    "If the model file exists and is loadable -> play. If the model file does not exist -> train, save & play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09701d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model file to load\n",
    "modelfile=\"OpenAI_Gym_Cartpole_deepRL.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4171b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyternotebook/virtualenv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# try loading model\n",
    "policy_net = load(torchdevice, modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc98163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900: ..................................................................................................."
     ]
    }
   ],
   "source": [
    "# if model could not be loaded -> train & save\n",
    "episode_rewards = None\n",
    "para = HyperParameters()\n",
    "if policy_net is None:\n",
    "    policy_net, episode_rewards = train(torchdevice, para, 10)\n",
    "    policy_net.save(modelfile)\n",
    "    plot_results(episode_rewards, para.n_episodes_ravg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62159a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n"
     ]
    }
   ],
   "source": [
    "# play\n",
    "episode_rewards = play(torchdevice, policy_net, para.n_episodes_ravg, 5, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34d8658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "best episode: 0 with duration 1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 \n",
      " 100 episode moving avg: 0.0\n"
     ]
    }
   ],
   "source": [
    "# print results of play\n",
    "if episode_rewards is not None:\n",
    "    plot_results(episode_rewards, para.n_episodes_ravg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
